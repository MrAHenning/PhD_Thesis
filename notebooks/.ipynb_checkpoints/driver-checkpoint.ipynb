{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('base': conda)",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Combining Classification-Centred and Relation-Based Argumentation Mining Methods\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<center><img src='../figures/pipeline-light-dark.png'></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Cython extensions are unavailable. Without them, this gensim functionality is disabled. If you've installed from a package, ask the package maintainer to include Cython extensions. If you're building gensim from source yourself, run `python setup.py build_ext --inplace` and retry. ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mmreader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMmReader\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.corpora._mmreader'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e3d818fcf2c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Gensim libraries                          [https://radimrehurek.com/gensim/]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[1;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1426\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mmreader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMmReader\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1428\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNO_CYTHON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Cython extensions are unavailable. Without them, this gensim functionality is disabled. If you've installed from a package, ask the package maintainer to include Cython extensions. If you're building gensim from source yourself, run `python setup.py build_ext --inplace` and retry. "
     ]
    }
   ],
   "source": [
    "# Native python 3.7.9 libraries             [https://docs.python.org/3.7/]\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "# NLTK Libraries                            [https://www.nltk.org/]\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Pandas library                            [https://pandas.pydata.org/docs/]\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy library                             [https://numpy.org/doc/stable/contents.html]\n",
    "import numpy as np\n",
    "\n",
    "# Gensim libraries                          [https://radimrehurek.com/gensim/]\n",
    "from gensim.models import Phrases\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "# Spacy libraries                           [https://spacy.io/api]\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Textblob libraries                        [https://textblob.readthedocs.io/en/dev/]\n",
    "from textblob import TextBlob\n",
    "\n",
    "# String similarity libaries                [https://pypi.org/project/strsim/#cosine-similarity]\n",
    "from similarity.cosine import Cosine      \n",
    "\n",
    "# Scikit-Learn libraries                    [https://scikit-learn.org/stable/]\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "source": [
    "## Define utility functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_files_in_list(file_list):\n",
    "    '''\n",
    "    :param file_list: A list of file names\n",
    "    :return:          No return\n",
    "\n",
    "    :description:     This function prints the contents of a list which holds filenames\n",
    "    :use:             Check the contents of a filename list\n",
    "    '''\n",
    "    for f in file_list:\n",
    "        print(f)\n",
    "\n",
    "\n",
    "def get_files(file_path):\n",
    "    '''\n",
    "    :param file_path: A string pointing to a directory containing files\n",
    "    :return:          A list of files\n",
    "\n",
    "    :description:     Returns a list of csv files, given a file path\n",
    "    :use:             Convenience function for obtaining files given a file path     \n",
    "    '''\n",
    "    file_list = [f for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "    return file_list\n",
    "\n",
    "def get_text_files(file_path):\n",
    "    '''\n",
    "    :param file_path: A string pointing to a directory containing files\n",
    "    :return:          A dataframe containing the text in the file (?)\n",
    "\n",
    "    :description:     \n",
    "    :use:\n",
    "    '''\n",
    "    text_dictionary = {}\n",
    "    file_list = [f for f in os.listdir(file_path) if f.endswith('.txt')]\n",
    "\n",
    "    for i, fc in enumerate(file_list):\n",
    "        print(\"Currently processing: {}\".format(fc))\n",
    "        with open(r\"{}\\{}\".format(file_path, fc), encoding='utf8') as f:\n",
    "            lines = f.read()\n",
    "            text_dictionary['text_{}'.format(i)] = lines\n",
    "\n",
    "    text_df = pd.DataFrame.from_dict(text_dictionary,\n",
    "                                     orient='index')\n",
    "    text_df.rename(columns={0: 'text'},\n",
    "                   inplace=True)\n",
    "    return text_df\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    :param treebank_tag: a string indicating the part of speech\n",
    "    :return:             a wrapper constant of the part of speech now as a type wordnet.<part-of-speech>\n",
    "\n",
    "    :description:\n",
    "    :use:\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "source": [
    "## Input Document"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<center><img src='../figures/pipeline-light-dark-inp.png'></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set data path to read the clean files from the IBM Debator Dataset\n",
    "data_path  = r'..\\IBM_Debater_(R)_CE-EMNLP-2015.v3\\articles'\n",
    "\n",
    "# Instantiate a blank list to hold the file names\n",
    "text_files = []\n",
    "\n",
    "# Iterate over all the files in the IBM Debator Dataset directory\n",
    "for f in os.listdir(data_path):\n",
    "\n",
    "    # Check if the file is a text file, e.g. \"ends with .txt\" extension\n",
    "    if f.endswith('.txt'):\n",
    "\n",
    "        # Add the file to the text files list\n",
    "        text_files.append(f)\n",
    "\n",
    "# Uncomment this code to see the list of text files\n",
    "# print_files_in_list(text_files)"
   ]
  },
  {
   "source": [
    "### Data Inspection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "source": [
    "## Stage A:      Segmentation and Unique Identification Number Assignment\n",
    "\n",
    "<center><img src='../figures/pipeline-light-dark-seg.png'></center>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clause_segmentation(parse_string):\n",
    "    '''\n",
    "    :param parse_string:    A flattened parse tree string of an individual sentence\n",
    "    :return:                A list of clauses\n",
    "\n",
    "    :description:           Takes a parse string as input, and segments text into clauses\n",
    "    :use:                   To segment text into individual clauses\n",
    "    '''\n",
    "\n",
    "    # Step 1: Form a parse tree from the flattened parse string\n",
    "    t = Tree.fromstring(parse_string)\n",
    "\n",
    "    # Instantiate a subtext list to hold the results\n",
    "    subtexts = []\n",
    "\n",
    "    # Step 2: Iterate through the tree, appending the sub-trees to subtext list if they indicate a clause (pos: S or SBAR)\n",
    "    for subtree in t.subtrees():\n",
    "        if subtree.label() == \"S\" or subtree.label() == \"SBAR\":\n",
    "            # print(\" \".join(subtree.leaves()))\n",
    "            subtexts.append(' '.join(subtree.leaves()))\n",
    "            \n",
    "\n",
    "    for i in reversed(range(len(subtexts) - 1)):\n",
    "        subtexts[i] = subtexts[i][0:subtexts[i].index(subtexts[i + 1])]\n",
    "\n",
    "    candidate_clauses = [x.split() for x in subtexts]\n",
    "    adjusted_candidate_clauses = []\n",
    "\n",
    "    for text in candidate_clauses:\n",
    "        # print(\"text: {} length: {}\".format(text, len(text)))\n",
    "        if len(text) > 1:\n",
    "            adjusted_candidate_clauses.append(text)\n",
    "\n",
    "    finalized_clauses = []\n",
    "    for text in adjusted_candidate_clauses:\n",
    "        finalized_clauses.append(' '.join(text))\n",
    "\n",
    "    # For consideration: add the previous 1 word string to previous list, e.g. ['which'] -> ['adds', 'to', 'their' ...]\n",
    "    return finalized_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_identifier(dictionary, entry_number):\n",
    "    '''\n",
    "    :param dictionary:      An ordered dictionary of paragraphs\n",
    "    :param entry_number:    The paragraph number\n",
    "    :return:                An ordered dictionary of sentences, whose keys contain unique identifiers and whose values are text fragments\n",
    "\n",
    "    :description:\n",
    "    :use:\n",
    "    '''\n",
    "\n",
    "    # Instantiate the CoreNLPParser, which parses the text and returns a parse tree of the text\n",
    "    parser = CoreNLPParser(url='http://localhost:9005')\n",
    "\n",
    "    # Instantiate a temporary unique identification number (UID) dictionary\n",
    "    temp_unique_identifier_dict = OrderedDict()\n",
    "\n",
    "    # Rename the entry number to make the programming / interpreation easier to work with\n",
    "    paragraph_identifier = entry_number\n",
    "\n",
    "    # Extract the text from the paragraph dictionary\n",
    "    paragraph_text = dictionary[paragraph_identifier]\n",
    "\n",
    "    # Step 1: tokenize the text into sentences\n",
    "    sentences = sent_tokenize(paragraph_text)\n",
    "\n",
    "    # Step 2: Iterate through the sentences and assign a number, starting with n = 1\n",
    "    sentence_dict = OrderedDict(enumerate(sentences, start=1))\n",
    "\n",
    "    # Step 3: Iterate through the new sentence dictionary created in Step 2, create the parse trees, extract the clauses, assign each clause a unique identification number\n",
    "    for sentence_identifier, sentence_text in sentence_dict.items():\n",
    "        try:\n",
    "            # Step 3.1: create the parse tree(s) of the sentence\n",
    "            trees = next(parser.raw_parse(sentence_text))\n",
    "\n",
    "            # Step 3.2: iterate through the tree(s)\n",
    "            for tr in trees:\n",
    "                tr1 = str(tr)\n",
    "\n",
    "                # create a tree from the flattened string\n",
    "                s1 = Tree.fromstring(tr1)\n",
    "\n",
    "                # generate the tree(s) production rules (documentation link: https://www.nltk.org/_modules/nltk/tree.html)\n",
    "                s2 = s1.productions()\n",
    "\n",
    "            # convert trees to string form\n",
    "            new_trees = [str(tr) for tr in trees]\n",
    "\n",
    "            # instantiate a list to trees after being joined\n",
    "            joined_tree_list = []\n",
    "\n",
    "            # Step 4: iterate over the new trees\n",
    "            for tr in new_trees:\n",
    "                joined_tree_list.append(tr.split())\n",
    "\n",
    "            # Step 5: instantiate a new container for new joined trees\n",
    "            new_joined_tree_list = []\n",
    "\n",
    "            for tr in joined_tree_list:\n",
    "                new_joined_tree_list.append(' '.join(tr))\n",
    "\n",
    "            # Step 6: segement into clauses and get the clause list from the sentence\n",
    "            clause_list = clause_segmentation(new_joined_tree_list[0])\n",
    "\n",
    "            # Step 7: iterate through the clause list, and assign a unique identification number\n",
    "            for clause_number, clause in enumerate(clause_list, start=1):\n",
    "                unique_identifier = '{}.{}.{}'.format(paragraph_identifier, sentence_identifier, clause_number)\n",
    "                temp_unique_identifier_dict[unique_identifier] = clause\n",
    "                # print('Clause Detection Successful')\n",
    "        \n",
    "        # If only one clause exists, update with unique identification number that ends with x.x.1 (e.g. paragraph.sentence.1)\n",
    "        except:\n",
    "            unique_identifier = '{}.{}.1'.format(paragraph_identifier, sentence_identifier)\n",
    "            temp_unique_identifier_dict[unique_identifier] = sentence_text\n",
    "            # print('Special Case Detected')\n",
    "\n",
    "    return temp_unique_identifier_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop: Iterate through text files in text_data directory and process files into their unique id form\n",
    "for file in text_files[:1]:\n",
    "\n",
    "    file_name = file\n",
    "\n",
    "    # Step 1: Open file\n",
    "    f = open(r'{}\\{}'.format(data_path, file_name), \"r\", encoding=\"utf8\")\n",
    "\n",
    "    # Step 2: Read raw data from file\n",
    "    lines = f.readlines()\n",
    "\n",
    "    # Step 3: Close file\n",
    "    f.close()\n",
    "\n",
    "    # Step 4: Iterate through file, eliminating spaces, which indicate paragraphs.\n",
    "    # Note: Each new line in the resulting list is a paragraph\n",
    "    lines = [line for line in lines if line != '\\n']\n",
    "\n",
    "    # Step 5: Create a paragraph dictionary, indexed by order of paragraph\n",
    "    paragraph_dict = OrderedDict(enumerate(lines, start=1))\n",
    "\n",
    "    # Step 6: Initialize an ordered dictionary to store results of unique identifiers and text\n",
    "    text_dictionary = OrderedDict()\n",
    "\n",
    "    # Step 7: Iterate through the paragraph dictionary and update the text_dictionary with results.\n",
    "    # Note: The final result is a dictionary of sentences, indexed by their unique identifiers.\n",
    "    for paragraph_number, paragraph_text in paragraph_dict.items():\n",
    "        text_dictionary.update(create_unique_identifier(paragraph_dict, paragraph_number))\n",
    "\n",
    "    # Step 8: Convert dictionary into pandas dataframe\n",
    "    text_df = pd.DataFrame.from_dict(text_dictionary, orient='index')\n",
    "    text_df.reset_index(inplace=True)\n",
    "\n",
    "    # Step 9: Rename the default columns to reflect the data\n",
    "    text_df.rename(columns={'index': 'unique_identifier', 0: 'text'}, inplace=True)\n",
    "    file_name = file_name.replace('.txt', '')\n",
    "\n",
    "    # Step 10: Save results as either a csv or excel file\n",
    "    # Note: Uncomment / comment out relevent line. CSV file is default\n",
    "    text_df.to_csv(\n",
    "        r'..\\results\\stage_a\\{}.csv'.format(\n",
    "            file_name), index=False)\n",
    "    \n",
    "    # Optional code for excel instead of csv output |\n",
    "    #------------------------------------------------\n",
    "    #\n",
    "    # text_df.to_excel(\n",
    "    #     r'C:\\Users\\andre\\PycharmProjects\\Hybrid_Argument_Mining\\post_18_month\\data\\result_excels\\{}_unique_id.xlsx'.format(\n",
    "    #         file_name), index=False)\n",
    "    # print(text_df.head())\n",
    "    # print('Finished processing: {}'.format(file_name))\n",
    "    text_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   unique_identifier                                               text\n",
       "0              1.1.1  Controversies over video games often center on...\n",
       "1              2.1.1  Video games have been studied for links to add...\n",
       "2              2.2.1  Earlier meta - analyses -LRB- an analysis of s...\n",
       "3              2.3.1  A 2001 study found that exposure to violent vi...\n",
       "4              2.4.1  A decrease in prosocial behavior -LRB- caring ...\n",
       "5              2.5.1  Another 2001 meta-analyses using similar metho...\n",
       "6              3.1.1  Many potential positive effects have been prop...\n",
       "7              3.2.1                      Recent research has suggested\n",
       "8              3.2.2  some violent video games may actually have a p...\n",
       "9              4.1.1  It has been argued there is generally a lack o...\n",
       "10             4.2.1  The most recent large scale meta-anlysis-- exa...\n",
       "11             4.3.1  However, this meta-analysis was severely criti...\n",
       "12             5.1.1  The Entertainment Software Association states ...\n",
       "13             6.1.1  In a survey of 1,102 teenagers aged 12 to 17, ...\n",
       "14             6.2.1  Three-quarters of parents who were surveyed sa..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>unique_identifier</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.1.1</td>\n      <td>Controversies over video games often center on...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.1.1</td>\n      <td>Video games have been studied for links to add...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.2.1</td>\n      <td>Earlier meta - analyses -LRB- an analysis of s...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.3.1</td>\n      <td>A 2001 study found that exposure to violent vi...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.4.1</td>\n      <td>A decrease in prosocial behavior -LRB- caring ...</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.5.1</td>\n      <td>Another 2001 meta-analyses using similar metho...</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.1.1</td>\n      <td>Many potential positive effects have been prop...</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.2.1</td>\n      <td>Recent research has suggested</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.2.2</td>\n      <td>some violent video games may actually have a p...</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>4.1.1</td>\n      <td>It has been argued there is generally a lack o...</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>4.2.1</td>\n      <td>The most recent large scale meta-anlysis-- exa...</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>4.3.1</td>\n      <td>However, this meta-analysis was severely criti...</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>5.1.1</td>\n      <td>The Entertainment Software Association states ...</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>6.1.1</td>\n      <td>In a survey of 1,102 teenagers aged 12 to 17, ...</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>6.2.1</td>\n      <td>Three-quarters of parents who were surveyed sa...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Re-examine the data after Segmentation\n",
    "text_df.head(15)"
   ]
  },
  {
   "source": [
    "## Stage B:      Classification\n",
    "\n",
    "<center><img src='../figures/pipeline-light-dark-class.png'></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "source": [
    "## Stage C: Templating\n",
    "\n",
    "<center><img src='../figures/pipeline-light-dark-temp.png'></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "np.random.seed(123456)\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "source": [
    "### Stage C.1:    Sentiment Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t-File 0 - Currently processing: clean_1.csv\n",
      "Saving to:  ..\\results\\stage_c_1/clean_1\n"
     ]
    }
   ],
   "source": [
    "# Declare file location of results from previous stage\n",
    "file_path = r\"..\\results\\stage_a\"\n",
    "\n",
    "# Set a save directory\n",
    "save_directory = r\"..\\results\\stage_c_1\"\n",
    "\n",
    "# Get files from previous stage\n",
    "file_list = get_files(file_path)\n",
    "\n",
    "# Iterate over files in file_list\n",
    "for i, fc in enumerate(file_list):\n",
    "\n",
    "    print(\"\\t-File {} - Currently processing: {}\".format(i, fc))\n",
    "    # Read the previoius file using pandas read_csv function\n",
    "    temp_frame = pd.read_csv(r'{}\\{}'.format(file_path, fc))\n",
    "\n",
    "    try:\n",
    "        # The code below uses TextBlob's \"sentiment\" method, which returns a named tuple\n",
    "        # \"Sentiment\" of the type \"Polarity / Subjectivity\"\n",
    "        '''\n",
    "        Example:\n",
    "            testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "            testimonial.sentiment\n",
    "            Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)\n",
    "            testimonial.sentiment.polarity\n",
    "            0.39166666666666666\n",
    "        '''\n",
    "        # Use textblob's sentiment polarity / subjectivity functions to apply subjectivity / polarity\n",
    "        temp_frame['sentiment_polarity'] = temp_frame['text'].apply(lambda t: TextBlob(t).sentiment[0])\n",
    "        temp_frame['sentiment_subjectivity'] = temp_frame['text'].apply(lambda t: TextBlob(t).sentiment[1])\n",
    "\n",
    "    # If the above code doesn't work, set a default value to \"skip\"\n",
    "    except:\n",
    "        temp_frame['sentiment_polarity']     = \"skip\"\n",
    "        temp_frame['sentiment_subjectivity'] = \"skip\"\n",
    "\n",
    "    # Housekeeping code to get file name and save stage progress using the same name, but to a different folder\n",
    "    save_file_name = fc[:len(fc) - 4]\n",
    "    temp_frame.to_csv(r'{}/{}.csv'.format(save_directory, save_file_name))\n",
    "    print(r\"Saving to:  {}/{}\".format(save_directory, save_file_name))"
   ]
  },
  {
   "source": [
    "### Stage C.2:    LDA / Topic Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data shape [Adjusted for NaN]: (1289, 2)\n",
      "Processing Tokens Processing LDA dictionary Processing LDA model via Gensim... Complete!\n",
      "Processing test string / checking for erros... \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   topic #  weight                                     words in topic\n",
       "3       14    0.34  0.005*\"vote\" + 0.004*\"party\" + 0.003*\"system\" ...\n",
       "2       13    0.31  0.015*\"israel\" + 0.007*\"report\" + 0.005*\"gaza\"...\n",
       "1        5    0.19  0.006*\"right\" + 0.004*\"condom\" + 0.004*\"law\" +...\n",
       "0        2    0.16  0.003*\"report\" + 0.003*\"right\" + 0.003*\"protes..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic #</th>\n      <th>weight</th>\n      <th>words in topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>3</td>\n      <td>14</td>\n      <td>0.34</td>\n      <td>0.005*\"vote\" + 0.004*\"party\" + 0.003*\"system\" ...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>13</td>\n      <td>0.31</td>\n      <td>0.015*\"israel\" + 0.007*\"report\" + 0.005*\"gaza\"...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>5</td>\n      <td>0.19</td>\n      <td>0.006*\"right\" + 0.004*\"condom\" + 0.004*\"law\" +...</td>\n    </tr>\n    <tr>\n      <td>0</td>\n      <td>2</td>\n      <td>0.16</td>\n      <td>0.003*\"report\" + 0.003*\"right\" + 0.003*\"protes...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "file_path = r'..\\lda_module_data\\lda_module_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "#print(\"Data shape [Raw]: {}\".format(data.shape))\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "# The following line can be commented out for testing purposes, else tests take far longer using the entire dataset\n",
    "# data = data.head(200)\n",
    "print(\"Data shape [Adjusted for NaN]: {}\".format(data.shape))\n",
    "\n",
    "data['sentences'] = data.text.apply(sent_tokenize)\n",
    "data['tokens_sentences'] = data['sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "data['POS_tokens'] = data['tokens_sentences'].apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['tokens_sentences_lemmatized'] = data['POS_tokens'].apply(\n",
    "    lambda list_tokens_POS: [\n",
    "                                [\n",
    "                                    lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1]))\n",
    "                                    if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "                                ]\n",
    "                                for tokens_POS in list_tokens_POS\n",
    "                            ]\n",
    "                                                               )\n",
    "\n",
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need',\n",
    "                    'like', 'make', 'see', 'want', 'come', 'take',\n",
    "                    'use', 'would', 'can']\n",
    "\n",
    "stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty',\n",
    "                    'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "\n",
    "my_stopwords = stopwords.words('English') + stopwords_verbs + stopwords_other\n",
    "\n",
    "data['tokens'] = data['tokens_sentences_lemmatized'].apply(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "data['tokens'] = data['tokens'].apply(lambda tokens: [token.lower() for token in tokens if token.isalpha()\n",
    "                                                        and token.lower() not in my_stopwords and len(token) > 1])\n",
    "\n",
    "\n",
    "print(\"Processing Tokens \", end=\"\")\n",
    "tokens = data['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])\n",
    "\n",
    "print(\"Processing LDA dictionary \", end=\"\")\n",
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]\n",
    "\n",
    "num_topics = 20\n",
    "print(\"Processing LDA model via Gensim... \", end=\"\")\n",
    "lda_model = models.LdaModel(corpus,\n",
    "                            num_topics=num_topics,\n",
    "                            id2word=dictionary_LDA,\n",
    "                            passes=4, alpha=[0.01] * num_topics,\n",
    "                            eta=[0.01] * len(dictionary_LDA.keys())\n",
    "                            )\n",
    "print(\"Complete!\")\n",
    "\n",
    "# Uncomment this code to print the topics\n",
    "# ---------------------------------------\n",
    "# print(\"Looking at Topics\")\n",
    "# for i, topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "#     print(str(i) + \": \" + topic)\n",
    "#     print()\n",
    "# print_long_line()\n",
    "\n",
    "# print(\"Allocating topics to documents\")\n",
    "# print(\"Article text: {}\".format(data.text.loc[0][:500]))\n",
    "# print(\"Topic: {}\".format(lda_model[corpus[0]]))\n",
    "\n",
    "# print(\"Predicting unseen text\")\n",
    "\n",
    "# Here is a sample text to use to check the model was trained correctly\n",
    "document = '''Controversies over video games often center on topics such as video game graphic violence, sex and sexism, violent and gory scenes, partial or full nudity, portrayal of criminal behavior, racism, and other provocative and objectionable material.\n",
    "\n",
    "Video games have been studied for links to addiction and aggression. Earlier meta-analyses (an analysis of several studies) were conflicting. A 2001 study found that exposure to violent video games causes at least a temporary increase in aggression and that this exposure correlates with aggression in the real world. A decrease in prosocial behavior (caring about the welfare and rights of others) was also noted. Another 2001 meta-analyses using similar methods and a more recent 2009 study focusing specifically on serious aggressive behavior concluded that video game violence is not related to serious aggressive behavior in real life.\n",
    "\n",
    "Many potential positive effects have been proposed. Recent research has suggested that some violent video games may actually have a prosocial effect in some contexts, for example, team play.\n",
    "\n",
    "It has been argued there is generally a lack of quality studies which can be relied upon and that the video game industry has become an easy target for the media to blame for many modern day problems. The most recent large scale meta-anlysis-- examining 130 studies with over 130,000 subjects worldwide-- concluded that exposure to violent video games causes both short term and long term aggression in players and decreases empathy and prosocial behavior. However, this meta-analysis was severely criticized in the same issue of the same journal for a number of methodological flaws, including failure to distinguish clinically valid from unstandardized aggression measures and for failing to solicit studies from researchers who have questioned whether causal links exist, thus biasing the sample of included studies.\n",
    "'''\n",
    "\n",
    "print('Testing / Error Checking ')\n",
    "tokens = word_tokenize(document)\n",
    "topics = lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20)\n",
    "frame = pd.DataFrame(\n",
    "                        [(el[0], round(el[1], 2), topics[el[0]][1]) for el in lda_model[dictionary_LDA.doc2bow(tokens)]],\n",
    "                        columns=['topic #', 'weight', 'words in topic']\n",
    "                    )\n",
    "\n",
    "frame.sort_values(by=\"weight\", inplace=True, ascending=False)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File 0 - Currently processing: clean_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Declare file location of results from previous stage\n",
    "file_path      = r'..\\results\\stage_c_1'\n",
    "\n",
    "# Set a save directory\n",
    "save_directory = r'..\\results\\stage_c_2'\n",
    "\n",
    "# Get files from previous stage\n",
    "file_list = get_files(file_path)\n",
    "\n",
    "for i, fc in enumerate(file_list):\n",
    "    print(\"File {} - Currently processing: {}\".format(i, fc))\n",
    "    temp_frame = pd.read_csv(r'{}\\{}'.format(file_path, fc))\n",
    "\n",
    "    temp_frame['lda_topics'] = \"None\"\n",
    "    for row in range(temp_frame.shape[0]):\n",
    "        try:\n",
    "            text = word_tokenize(temp_frame.iloc[row]['text'])\n",
    "            topics = lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20)\n",
    "            lda_frame = pd.DataFrame(\n",
    "                                        [(el[0], round(el[1], 2), topics[el[0]][1]) for el in lda_model[dictionary_LDA.doc2bow(tokens)]],\n",
    "                                        columns=['topic #', 'weight', 'words in topic']\n",
    "                                     )\n",
    "\n",
    "            topic_string = ''\n",
    "\n",
    "            for item in lda_frame[\"topic #\"].tolist():\n",
    "                topic_string = topic_string + '{} '.format(item)\n",
    "            \n",
    "            temp_frame.loc[row, 'lda_topics'] = topic_string\n",
    "        except:\n",
    "            print(\"Error detected, leaving row as 'None'\")\n",
    "\n",
    "    # print(temp_frame.head())\n",
    "    save_file_name = fc[:len(fc) - 4]\n",
    "    temp_frame.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    temp_frame.to_csv(r'{}/{}.csv'.format(save_directory, save_file_name))"
   ]
  },
  {
   "source": [
    "### Stage C.3:    Similarity Measures"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File 0 - Currently processing: clean_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Declare file location of results from previous stage\n",
    "file_path = r'..\\results\\stage_c_2'\n",
    "\n",
    "# Set a save directory\n",
    "save_directory = r'..\\results\\stage_c_3'\n",
    "\n",
    "# Get files from previous stage\n",
    "file_list = get_files(file_path)\n",
    "\n",
    "for i, fc in enumerate(file_list):\n",
    "    print(\"File {} - Currently processing: {}\".format(i, fc))\n",
    "    temp_frame = pd.read_csv(r'{}\\{}'.format(file_path, fc))\n",
    "    save_frame = temp_frame.copy()\n",
    "    temp_frame.drop(['Unnamed: 0','sentiment_polarity', 'sentiment_subjectivity', 'lda_topics'], axis=1, inplace=True)\n",
    "\n",
    "    unique_identifiers = temp_frame['unique_identifier'].tolist()\n",
    "    records_dict = temp_frame.to_dict('records')\n",
    "    text_dict = dict()\n",
    "\n",
    "    for uid, record in zip(unique_identifiers, records_dict):\n",
    "        text_dict[uid] = record['text']\n",
    "\n",
    "\n",
    "    # This code can also be used to calculate Cosine similarities, using sklearn, Non-negative matrix factorization, and pandas \"dot\"              function\n",
    "    # Note: This method is an approximation and has an element of randomness. \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    # documents = new_frame['text'].tolist()\n",
    "    # print(documents)\n",
    "\n",
    "    # tfidf = TfidfVectorizer()\n",
    "    # csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "    # model = NMF(n_components=20)\n",
    "    # model.fit(csr_mat)\n",
    "    # nmf_features = model.transform(csr_mat)\n",
    "    # norm_features = normalize(nmf_features)\n",
    "\n",
    "    # print(nmf_features)\n",
    "\n",
    "    # df = pd.DataFrame(nmf_features, index=new_frame_list)\n",
    "    # article = df.loc['1.1.1']\n",
    "    # similarities = df.dot(article)\n",
    "\n",
    "    # print(similarities)\n",
    "\n",
    "    cosine = Cosine(2)\n",
    "\n",
    "    for uid_1 in unique_identifiers:\n",
    "        similarity_list = []\n",
    "        new_column_name = 'cos_sim_to_{}'.format(uid_1)\n",
    "        for uid_2 in unique_identifiers:\n",
    "            \n",
    "            # Comment out the three lines below for code testing\n",
    "            # print(\"comparing {} to {}\".format(uid_1, uid_2))\n",
    "            # print(\"Text 1 --> {}\".format(newer_dict[uid_1]))\n",
    "            # print(\"Text 1 --> {}\".format(newer_dict_two[uid_2]))\n",
    "\n",
    "            p0 = cosine.get_profile(text_dict[uid_1])\n",
    "            p1 = cosine.get_profile(text_dict[uid_2])\n",
    "            measure = cosine.similarity_profiles(p0, p1)\n",
    "            similarity_list.append(measure)\n",
    "\n",
    "        temp_frame[new_column_name] = similarity_list\n",
    "    \n",
    "    save_frame = save_frame.merge(temp_frame, how='left')\n",
    "    save_frame.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    save_file_name = fc[:len(fc) - 4]\n",
    "    # temp_frame.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    save_frame.to_csv(r'{}/{}.csv'.format(save_directory, save_file_name))\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Stage C.4:    Create Template"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stage D:      Adjustment\n",
    "\n",
    "<center><img src='../figures/pipeline-light-dark-adj.png'></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Output\n",
    "\n",
    "<center><img src='../figures/pipeline-light-dark-out.png'></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}